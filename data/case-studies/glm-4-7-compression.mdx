---
title: 'Compressing 363B Parameter MoE Models with REAP Pruning and INT4 Quantization'
client: 'Personal Research'
date: '2025-01-07'
summary: 'End-to-end documentation of compressing GLM-4.7 from 668GB to 92GB using REAP expert pruning and AutoRound INT4 quantization, achieving 7× reduction while maintaining model quality.'
tags: ['AI', 'LLM', 'Model Compression', 'MoE', 'Quantization', 'Python']
images: ['/static/images/case-studies/glm-4-7-compression.png']
featured: true
metrics:
  - label: 'Original Model Size'
    value: '668GB'
  - label: 'Compressed Size'
    value: '92GB'
  - label: 'Compression Ratio'
    value: '7×'
  - label: 'Infrastructure'
    value: '8×H200 GPUs'
  - label: 'Total Runtime'
    value: '~8 hours'
  - label: 'Estimated Cost'
    value: '$100-130'
---

## Executive Summary

This case study documents the compression of **GLM-4.7**, a 363 billion parameter Mixture-of-Experts (MoE) language model, from an unwieldy 668GB BF16 format to a deployable 92GB INT4 representation. By combining **REAP expert pruning** (from Cerebras Research) with **AutoRound quantization**, we achieved a 7× size reduction that enables inference on consumer hardware while preserving model quality.

The compression pipeline required approximately 80 hours on an 8×H200 GPU cluster, costing roughly $1000 USD using spot pricing. The resulting models checkpoints is available on HuggingFace under the [0xSero](https://huggingface.co/0xSero) namespace.
This pipeline could easily have taken 5 times less, or ten times more. Depending on experience, readiness, and size of the calibration dataset.

## The Problem: Giant Models Can't Run On Consumer Hardware

Large language models development has accelerated dramatically, with frontier models exceeding hundreds of billions of parameters. GLM-4.7 exemplifies this trend:

| Specification         | Value                    |
| --------------------- | ------------------------ |
| Parameters            | 363 billion              |
| Experts               | 160 (sparse activation)  |
| Architecture          | Mixture-of-Experts (MoE) |
| Full Precision (BF16) | ~668 GB                  |
| Inference VRAM (FP16) | ~1 TB                    |

This is a significant deployment challenge. Even with access to expensive GPU clusters, loading the model consumes all available VRAM, leaving no headroom for inference batching, KV cache, or overhead. The practical implications:

- **Datacenter access required**: No single machine can load the model
- **High operational costs**: Running on cloud GPUs at scale is expensive
- **Limited accessibility**: Researchers and small teams cannot experiment
- **Environmental impact**: Massive compute requirements per inference

## Why MoE Models Are Ideal for Compression

Mixture-of-Expert architectures present a unique opportunity for compression. Unlike dense models where every parameter participates in every forward pass, MoE models route tokens processing through a subset of experts:

```
┌─────────────────────────────────────────────────────────────┐
│                    GLM-4.7 Architecture                     │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐      │
│  │  Expert 1   │    │  Expert 2   │    │  Expert 3   │      │
│  │  (active)   │    │  (active)   │    │  (inactive) │      │
│  └─────────────┘    └─────────────┘    └─────────────┘      │
│         │                 │                 │               │
│         ▼                 ▼                 ○               │
│    ┌──────────────────────────────────────────────────┐     │
│    │              Router/Gate Mechanism               │     │
│    │         Selects top-k experts per token          │     │
│    └──────────────────────────────────────────────────┘     │
│                          │                                  │
│                          ▼                                  │
│                  ┌─────────────┐                            │
│                  │   Output    │                            │
│                  └─────────────┘                            │
└─────────────────────────────────────────────────────────────┘
```

Key observations about MoE redundancy:

1. **Not all experts are equal**: Router biases means some experts are selected far more frequently
2. **Expert specialization**: Certain experts specialize in overlapping capabilities
3. **Submodular contribution**: Removing an expert's contribution diminishes predictably with depth
4. **Structural sparsity**: The model is already designed for sparse activation

REAP (Router weighted expert activation pruning) exploits these properties by measuring expert importance through calibration with specific datasets, then removing the least critical experts entirely.

## The Solution: Two-Stage Compression Pipeline

### Stage 1: REAP Expert Pruning

REAP operates in three phases:

1. **Calibration Forward Pass**: Run representative samples through the model, collecting activation statistics for each expert
2. **Importance Scoring**: Compute saliency scores for each expert based on activation norms and gate values
3. **Expert Removal**: Remove the least important experts

The critical insight is that expert importance is **task-dependent**. For our compression target (code generation + function calling + agentic workflows), we curated a calibration dataset matching these use cases.

#### Calibration Dataset Composition

| Dataset                              | Samples  | Purpose                        |
| ------------------------------------ | -------- | ------------------------------ |
| theblackcat102/evol-codealpaca-v1    | 700      | Code generation                |
| Salesforce/xlam-function-calling-60k | 330      | Function calling / tool use    |
| SWE-bench/SWE-smith-trajectories     | 330      | Agentic multi-turn workflows   |
| **Total**                            | **1360** | Domain-appropriate calibration |

This mirrors the approach in the original Cerebras REAP paper, which found that calibration domain significantly impacts downstream quality. Our dataset balances the three primary use cases for compressed GLM-4.7.

#### Command and Configuration

```bash
python /mnt/work/reap/src/reap/prune.py \
  --model-name /mnt/work/model/GLM-4.7 \
  --dataset-name 0xSero/glm47-reap-calibration-v2 \
  --compression-ratio 0.50 \
  --seed 42 \
  --distance_measure angular
```

Key parameters:

- **compression-ratio**: Proportion of experts retained (0.50 = 50% of experts kept)
- **seed**: Reproducible randomness for observation caching
- **distance-measure**: Angular (angular distance between expert outputs vectors subspaces)

#### Runtime Observations

| Phase                                 | Duration    |
| ------------------------------------- | ----------- |
| Observation collection (1360 samples) | ~12.5 hours |
| Pruning and model repacking           | ~1.5 hours  |
| **Total REAP 40% prune**              | **~14h**    |

Once observations are cached at `/root/artifacts/GLM-4.7/glm47-reap-calibration-v2/all/observations_1360_angular-seed_42.pt`, additional prune ratios (35%, 40%, 45%, 50%) can be computed in minutes rather than hours.

### Stage 2: AutoRound INT4 Quantization

Pruning alone reduces model size but not sufficiently for practical deployment. We applied **AutoRound**, a GPTQ-compatible quantization method that:

- Compresses weights to 4-bit integers
- Uses group-wise quantization (128 parameters per group)
- Applies asymmetric quantization with GPTQ-style optimization
- Preserves activation precision (W4A16 format)

#### AutoRound Configuration

```bash
auto-round \
  --model /path/to/pruned-model \
  --bits 4 \
  --group_size 128 \
  --format auto_gptq \
  --output_dir /mnt/work/outputs/GLM-4.7-REAP-50-W4A16
```

#### Quantization Runtime

| Phase                                  | Duration    |
| -------------------------------------- | ----------- |
| Per-layer quantization (~90-92s/layer) | ~3-4 hours  |
| Model compilation and export           | ~10 minutes |
| **Total AutoRound pass**               | **~4h 10m** |

### Combined Results

| Model                   | Size    | VRAM Required | Compression |
| ----------------------- | ------- | ------------- | ----------- |
| GLM-4.7 Original (BF16) | 717 GB  | ~1 TB         | 1×          |
| GLM-4.7-REAP-50 (BF16)  | ~345 GB | ~500 GB       | 1.9×        |
| GLM-4.7-REAP-50-W4A16   | 92 GB   | ~120 GB       | **7.3×**    |

The 92GB final model can run inference on a high end macbook or 8x 3090s, making it accessible to independent researchers and small teams deployments—hardware costing ~$3,000 rather than the ~$300,000 required for 8×H200 cluster access.

## Infrastructure and Economics

### Compute Environment

All experiments ran on **Prime Intellect** cloud infrastructure:

| Resource   | Specification                                          |
| ---------- | ------------------------------------------------------ |
| GPUs       | 8× NVIDIA H200 (143GB each, 1.15TB total VRAM)         |
| Pricing    | ~$13/hour (spot pricing)                               |
| SSH Access | `ssh -i ~/.ssh/id_rsa_prime root@86.38.238.92 -p 1234` |

### Spot Instance Strategy

The pipeline is designed to be **checkpoint-resumable**. Spot instances preemption is a realistic concern:

1. **Observation caching**: The 50GB observation file persists to disk, allowing resume from calibration phase
2. **Async uploads**: Model checkpoints uploads triggers immediately upon completion, before instance termination risk
3. **DONE markers**: Pipeline status tracked via `.DONE` files for resume capability

If preemption occurs:

- Launch new 8×H200 spot instance
- Observations file persists (mounted storage)
- Pipeline automatically resumes from last checkpoint

## Available Model Checkpoints

All compressed models variants are available on HuggingFace:

| Repository                                                                                  | Description                       |
| ------------------------------------------------------------------------------------------- | --------------------------------- |
| [0xSero/GLM-4.7-REAP-40](https://huggingface.co/0xSero/GLM-4.7-REAP-40)                     | 40% expert retention (BF16)       |
| [0xSero/GLM-4.7-REAP-40-W4A16](https://huggingface.co/0xSero/GLM-4.7-REAP-40-W4A16)         | 40% + 4-bit quantization (~108GB) |
| [0xSero/GLM-4.7-REAP-50](https://huggingface.co/0xSero/GLM-4.7-REAP-50)                     | 50% expert retention (BF16)       |
| [0xSero/GLM-4.7-REAP-50-W4A16](https://huggingface.co/0xSero/GLM-4.7-REAP-50-W4A16)         | 50% + 4-bit quantization (~92GB)  |
| [0xSero/glm47-reap-calibration-v2](https://huggingface.co/0xSero/glm47-reap-calibration-v2) | Calibration dataset               |
| [0xSero/glm47-reap-observations](https://huggingface.co/0xSero/glm47-reap-observations)     | Cached observations               |

### Recommended Configuration

**For most use cases**: GLM-4.7-REAP-40-W4A16 offers the optimal balance of quality and size. At 40% expert retention, quality degradation is minimal across most benchmarks while delivering a compact 108GB model.

**For extreme VRAM constraints**: GLM-4.7-REAP-50-W4A16 at 92GB can run on single high-VRAM GPUs, but expect measurable quality degradation on reasoning-heavy tasks.

## Quality Recovery: Optional Distillation

If quantization introduces unacceptable quality degradation, **self-distillation with LoRA** can recover performance:

1. **Generate synthetic data** using the BF16 pruned model (teacher) via Magpie-style prompting
2. **Train LoRA adapter** on the quantized model (student) with KL divergence loss against teacher
3. **Apply adapter** during inference to improve output quality

This is the approach used by Apple and Ellora for post-training recovery. Most gains come from the first few thousand samples—the distillation curve exhibits diminishing returns rapidly.

## Technical Challenges and Solutions

### Challenge 1: Calibration Domain Mismatch

**Problem**: Generic calibration datasets may not capture the expert specializations relevant to our target use cases.

**Solution**: We selected a domain-specific dataset combining code generation (evol-codealpaca), function calling (xlam), and agentic trajectories (SWE-smith). This mirrors the intended deployment scenarios and ensures REAP scores experts based on relevant performance.

### Challenge 2: Multi-GPU Inference Compatibility

**Problem**: Compressed models checkpoint spanned 8 GPUs, but evaluation frameworks (evalplus) couldn't handle distributed inference.

**Solution**: This remains a limitation for automated benchmarking. Manual evaluation or single-GPU deployment required for quantitative assessment. Future work involves distributed inference infrastructure for compressed MoE models.

### Challenge 3: Spot Instance Preemption

**Problem**: Long-running jobs (8 hours) face inevitable preemption risk on spot instances.

**Solution**: Designed pipeline with checkpoint resumability. Observations file (~50GB) cached to persistent storage. Async upload to HuggingFace triggered immediately upon checkpoint generation, before upload completes instance termination risk.

## Lessons Learned

1. **Cache observations strategically**: The 50GB observation file is reusable across multiple prune ratios experiments. Build once, test many configurations rapidly.

2. **40% is the sweet spot**: For MoE compression, removing 40-45% of experts provides optimal quality/size tradeoff. 50% is aggressive but useful for extreme constraints scenarios.

3. **Spot pricing changes economics**: At $13/hr versus on-demand pricing (~$40-50/hr), full pipeline cost drops from $400+ to ~$100. Essential for research budgets management.

4. **Quantization after pruning compounds gains**: W4A16 quantization after pruning achieves better compression than either alone. The weight distribution after pruning appears more amenable to quantization.

5. **Upload before termination**: Always trigger model upload before final checkpoint completes. Lost upload progress is expensive lost time.

## Future Work

1. **Evaluate compressed variants**: Systematic benchmark comparison across MMLU, HumanEval, and agentic benchmarks suite

2. **Distillation recovery**: Implement LoRA-based self-distillation pipeline for quality recovery on aggressive compression

3. **Additional prune ratios**: Complete 35%, 45% variants for finer granularity quality/size tradeoff curve

4. **GGUF conversion**: Export quantized model to GGUF format for local inference via llama.cpp on consumer hardware

5. **Distributed inference**: Infrastructure for compressed MoE distributed inference across multiple consumer GPUs

## References

- [REAP (Cerebras)](https://arxiv.org/abs/2510.13999)
- [AutoRound Quantization](https://github.com/intel/auto-round)
- [Pime Intellect](https://www.primeintellect.ai/)
- [GLM-4.7 Model](https://huggingface.co/zai-org/GLM-4.7)
- [HuggingFace Models](https://huggingface.co/0xSero)

---

_This compression work was conducted on Prime Intellect infrastructure. Model checkpoints available under permissive licensing on HuggingFace._
