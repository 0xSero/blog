---
title: 'Part 1: How I Actually Work With AI â€” The Workstyle Report'
date: '2025-01-17'
tags: ['Productivity', 'Self-Analysis', 'Metrics', 'Reflection']
part: 1
sequence: workstyle-report-and-sero-nouscoder
draft: false
summary: 'I ran a privacy preserving analysis on 809 conversations. The results were humbling, surprising, and exactly what I needed to see.'
---

I extracted 727MB of conversations from Cursor, Claude, and Codex. Then I ran a privacy preserving analysis on 809 conversations spanning 4 months. What I found changed how I think about my own workflow.

## The Setup

I built a pipeline that:

1. Extracted conversations from local AI tools
2. Analyzed behavioral signals without storing raw text
3. Enforced k-anonymity (k=10) on all breakdowns
4. Generated composite indices from keyword patterns

Total: 19,539 user messages, 11,726 assistant messages, 16,484 tool uses.

## The Strengths According to Data

### 1. Bias toward actionability

My spec completeness score averaged 0.275. That is constraint/step framing. I frame problems with clear next steps rather than abstract discussions.

> "Fix the TypeScript error in file X. Error: Property 'toString' does not exist on type 'never'."

Not "I have a problem." Just the problem, the file, the error, the ask.

### 2. Debug loops over single-shot asks

40.3% of my conversations included error sharing. I iterate. I share the error, get a fix, encounter the next error, share that, repeat.

The debug maturity index was low (0.056), but that is because I rarely include minimal repro language. I jump straight to the error.

### 3. Multi-system orchestration

Top languages by conversation:

- JavaScript: 182 conversations
- Bash: 175
- Go: 156
- TypeScript: 133
- SQL: 95

I move between frontend (React, Next.js), backend (Go, TypeScript), infrastructure (Docker, AWS), automation (n8n, Slack integrations).

### 4. Testing awareness

43.6% of conversations mentioned testing. I do not think of myself as test-disciplined. But I mention tests frequently even if I am bad at writing them.

## The Improvement Opportunities

The report was honest. Here are the gaps.

### 1. Minimal reproduction

User conversations with repro language: 0.4%

That means 99.6% of the time, I ask for help without providing a minimal reproducible example. I just dump the error and expect the AI to figure it out.

### 2. Test discipline

Testing mentions: 43.6%
Testing discipline index: 0.096

I talk about tests. I rarely write them. The assistant's testing discipline score (0.173) was nearly double mine. The AI tests more than I do.

This is a pattern. I delegate testing. Which means I do not really own quality.

### 3. Security hygiene

Risky disclosure signals: 32.5%

32.5% of my conversations contained signals that could indicate sensitive data exposure. API keys, environment variables, addresses. The heuristics do not store tokens, but they flag patterns.

### 4. Acceptance criteria

When delegating multi-file changes, I rarely specify acceptance criteria upfront. This shows up in the spec completeness index being barely above 0.25.

The data caught what I knew was true. I get vague, then iterate, instead of being clear, then shipping.

## The Time-Series Trends

The weekly breakdown told a story.

| Week       | n   | Spec  | Debug | Test  | Security | Risky% | Tool Uses |
| ---------- | --- | ----- | ----- | ----- | -------- | ------ | --------- |
| 2025-09-22 | 19  | 0.487 | 0.049 | 0.357 | 0.113    | 42.1%  | 0         |
| 2025-10-13 | 156 | 0.323 | 0.073 | 0.078 | 0.036    | 20.5%  | 0         |
| 2025-12-22 | 117 | 0.237 | 0.073 | 0.110 | 0.093    | 41.9%  | 6,261     |
| 2026-01-05 | 70  | 0.213 | 0.050 | 0.098 | 0.118    | 38.6%  | 3,044     |

### Volume correlates with tool use

In late December, tool uses exploded from 0 to 6,261. That corresponds with Claude Code integration. I went from conversational AI to agentic AI.

### Spec completeness degrades with volume

As conversations increased, spec completeness dropped. More volume, less care. This is the classic trade-off. Speed over quality.

### Security awareness is cyclical

Security mentions spiked in weeks 50 (0.135), 47 (0.133), and 46 (0.118). These correlate with integration work, adding new tools, connecting new systems.

## The Composite Indices

The report included heuristic proxies (0-1 scale):

| Index                 | User (Me) | Assistant |
| --------------------- | --------- | --------- |
| Spec completeness     | 0.275     | 0.277     |
| Debug maturity        | 0.056     | 0.040     |
| Testing discipline    | 0.096     | 0.173     |
| Security awareness    | 0.058     | 0.064     |
| Architecture-thinking | 0.168     | 0.129     |

The assistant beats me on testing (0.173 vs 0.096). It beats me on security (0.064 vs 0.058). But I beat it on architecture (0.168 vs 0.129).

This is telling. I think about structure more than execution. The AI executes better than I do.

## What This Taught Me

### 1. I am an action-oriented, high-volume developer

809 conversations in 4 months. That is about 6 to 7 conversations per day. I use AI as a thinking partner, not just a code generator.

### 2. I iterate more than I plan

40% error sharing but only 0.4% repro language. I debug in public. This is efficient for me but exhausting for collaborators.

### 3. I delegate testing, rarely doing it myself

Talking about tests (43.6%) is not writing tests. The assistant has higher testing discipline than I do.

### 4. I leak too much sensitive data

32.5% of conversations contained risky disclosure signals. This is a concrete, measurable hygiene problem.

### 5. The assistant complements my weaknesses

The AI has higher testing discipline and security awareness. It catches what I miss. This is the right mental model. AI as amplifier, not replacement.

## The Action Plan

Based on the data, here is what I am changing.

### 1. Add repro template

```
## Error
[exact error message]

## Expected
[what should happen]

## Actual
[what actually happens]

## Minimal repro
[shortest code that demonstrates the issue]
```

### 2. Test discipline checklist

```
- [ ] Write test before fix
- [ ] Run tests after fix
- [ ] Add regression check
- [ ] Document test coverage
```

### 3. Security scan before commit

```bash
# Pre-commit hook
grep -r "sk-\\|pk-\\|0x[a-fA-F0-9]{64}" --exclude-dir=node_modules
```

### 4. Acceptance criteria for delegation

```
## Deliverables
- [ ] File A modified
- [ ] File B created
- [ ] Tests pass

## Acceptance
- [ ] Compiles without errors
- [ ] Handles edge case X
- [ ] Matches style of existing code
```

## The Bigger Picture

809 conversations is a lot. It is also just 4 months. At this rate, I will have 2,400 conversations by the end of the year.

The question is not whether I use AI. I clearly do, aggressively.

The question is: am I using it to grow, or to avoid growth?

The data suggests both. I ship faster (actionability is high). I think less (spec completeness is low). I delegate testing (assistant beats me).

This is a trade-off. Every speedup has a cost. Every delegation has a gap.

## What I Would Tell Someone Else

If you are analyzing your own AI usage:

1. Extract your data. It is harder than it sounds, but worth it
2. Run privacy-preserving analysis. Do not store raw conversations
3. Look for patterns, not scores. The indices are heuristics. Trends are truth
4. Find gaps. Where are you delegating what you should own?
5. Set concrete changes. Vague improvement goals produce vague results

## The Verdict

The workstyle report was humbling. It confirmed suspicions I had and revealed blind spots I did not.

I am not a great debugger (low debug maturity). I do not write tests (low testing discipline). I leak sensitive data (high risky disclosure).

But I am action-oriented (high constraint framing), multi-system (broad language distribution), and iterative (high error sharing).

The profile is not good or bad. It is just true.

And now that it is true, I can work with it.

---

Continue to Part 2: Training My Own Coding Model, where I take these insights and my actual conversations to train sero-nouscoder.

_Data: 809 conversations, 19,539 messages, 16,484 tool uses, 4 months of AI-assisted development. Analysis: privacy-preserving, k-anonymous, heuristic-based. Verdict: honest._
