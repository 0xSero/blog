---
title: '809 Conversations, 16K Tool Uses, and One Honest Look in the Mirror'
date: '2025-01-17'
tags: ['Productivity', 'Self-Analysis', 'Metrics', 'Reflection']
draft: false
summary: 'I ran an analysis on 809 conversations with AI assistants. The results were humbling, surprising, and exactly what I needed to see.'
---

I extracted 727MB of conversations from Cursor, Claude, and Codex. Then I ran a privacy-preserving analysis on 809 conversations spanning 4 months. What I found changed how I think about my own workflow.

## The Setup

I built a pipeline that:

1. Extracted conversations from local AI tools
2. Analyzed behavioral signals without storing raw text
3. Enforced k-anonymity (k=10) on all breakdowns
4. Generated composite indices from keyword patterns

Total: 19,539 user messages, 11,726 assistant messages, 16,484 tool uses.

What follows is what the data said about me.

## The Strengths (According to Data)

**1. Bias toward actionability**

My spec completeness score averaged 0.275. That doesn't sound impressive until you realize it's "constraint/step framing"—I frame problems with clear next steps rather than abstract discussions.

Example from my conversations:

> "Fix the TypeScript error in file X. Error: Property 'toString' does not exist on type 'never'."

Not "I have a problem." Just the problem, the file, the error, the ask.

**2. Debug loops over single-shot asks**

40.3% of my conversations included error sharing. This isn't a bug—it's a feature. I iterate. I share the error, get a fix, encounter the next error, share that, repeat.

The debug maturity index was low (0.056), but that's because I rarely include "minimal repro" language. I jump straight to the error. The data called this out correctly.

**3. Multi-system orchestration**

Top languages by conversation:

- JavaScript: 182 conversations
- Bash: 175
- Go: 156
- TypeScript: 133
- SQL: 95

I'm not a specialist. I'm someone who moves between frontend (React, Next.js), backend (Go, TypeScript), infrastructure (Docker, AWS), and automation (n8n, Slack integrations).

**4. Testing awareness**

43.6% of conversations mentioned testing. This surprised me. I don't think of myself as test-disciplined. But apparently, I mention tests frequently—even if I'm bad at actually writing them.

## The Improvement Opportunities

The report was honest. Here are the gaps:

**1. Minimal reproduction**

User conversations with repro language: 0.4%

That means 99.6% of the time, I ask for help without providing a minimal reproducible example. I just dump the error and expect the AI to figure it out.

The report said it nicely: "Include explicit minimal repro + expected/actual more consistently."

**2. Test discipline**

Testing mentions: 43.6%
Testing discipline index: 0.096

I talk about tests. I rarely write them. The assistant's testing discipline score (0.173) was nearly double mine. The AI tests more than I do.

This is a pattern: I delegate testing. Which means I don't really own quality.

**3. Security hygiene**

Risky disclosure signals: 32.5%

32.5% of my conversations contained signals that could indicate sensitive data exposure. API keys, environment variables, addresses. The heuristics don't store tokens, but they flag the patterns.

The report was polite: "Reduce sensitive-data-like disclosure signals."

**4. Acceptance criteria**

When delegating multi-file changes, I rarely specify acceptance criteria upfront. This shows up in the spec completeness index being barely above 0.25.

The data caught what I knew was true: I get vague, then iterate, instead of being clear, then shipping.

## The Time-Series Trends

The weekly breakdown told a story:

| Week       | n   | Spec  | Debug | Test  | Security | Risky% | Tool Uses |
| ---------- | --- | ----- | ----- | ----- | -------- | ------ | --------- |
| 2025-09-22 | 19  | 0.487 | 0.049 | 0.357 | 0.113    | 42.1%  | 0         |
| 2025-10-13 | 156 | 0.323 | 0.073 | 0.078 | 0.036    | 20.5%  | 0         |
| 2025-12-22 | 117 | 0.237 | 0.073 | 0.110 | 0.093    | 41.9%  | 6,261     |
| 2026-01-05 | 70  | 0.213 | 0.050 | 0.098 | 0.118    | 38.6%  | 3,044     |

Two patterns emerge:

**1. Volume correlates with tool use**

In late December, tool uses exploded from 0 to 6,261. That corresponds with the Claude Code integration. I went from conversational AI to agentic AI.

**2. Spec completeness degrades with volume**

As conversations increased, spec completeness dropped. More volume, less care. This is the classic trade-off: speed over quality.

**3. Security awareness is cyclical**

Security mentions spiked in weeks 50 (0.135), 47 (0.133), and 46 (0.118). These correlate with integration work—adding new tools, connecting new systems.

## The Composite Indices

The report included heuristic proxies (0-1 scale):

| Index                 | User (Me) | Assistant |
| --------------------- | --------- | --------- |
| Spec completeness     | 0.275     | 0.277     |
| Debug maturity        | 0.056     | 0.040     |
| Testing discipline    | 0.096     | 0.173     |
| Security awareness    | 0.058     | 0.064     |
| Architecture-thinking | 0.168     | 0.129     |

The assistant beats me on testing (0.173 vs 0.096). It beats me on security (0.064 vs 0.058). But I beat it on architecture (0.168 vs 0.129).

This is telling: I think about structure more than execution. The AI executes better than I do.

## What This Taught Me

**1. I'm an action-oriented, high-volume developer**

809 conversations in 4 months. That's ~6-7 conversations per day. I use AI as a thinking partner, not just a code generator.

**2. I iterate more than I plan**

40% error sharing but only 0.4% repro language. I debug in public. This is efficient for me but exhausting for collaborators.

**3. I delegate testing, rarely doing it myself**

Talking about tests (43.6%) != writing tests. The assistant has higher testing discipline than I do.

**4. I leak too much sensitive data**

32.5% of conversations contained risky disclosure signals. This is a concrete, measurable hygiene problem.

**5. The assistant complements my weaknesses**

The AI has higher testing discipline and security awareness. It catches what I miss. This is the right mental model: AI as amplifier, not replacement.

## The Action Plan

Based on the data, here's what I'm changing:

**1. Add repro template**

```
## Error
[exact error message]

## Expected
[what should happen]

## Actual
[what actually happens]

## Minimal repro
[shortest code that demonstrates the issue]
```

**2. Test discipline checklist**

- [ ] Write test before fix
- [ ] Run tests after fix
- [ ] Add regression check
- [ ] Document test coverage

**3. Security scan before commit**

```bash
# Pre-commit hook
grep -r "sk-\|pk-\|0x[a-fA-F0-9]{64}" --exclude-dir=node_modules
```

**4. Acceptance criteria for delegation**

```
## Deliverables
- [ ] File A modified
- [ ] File B created
- [ ] Tests pass

## Acceptance
- [ ] Compiles without errors
- [ ] Handles edge case X
- [ ] Matches style of existing code
```

## The Bigger Picture

809 conversations is a lot. It's also just 4 months. At this rate, I'll have 2,400 conversations by the end of the year.

The question isn't whether I use AI. I clearly do—aggressively.

The question is: am I using it to grow, or to avoid growth?

The data suggests both. I ship faster (actionability is high). I think less (spec completeness is low). I delegate testing (assistant beats me).

This is the trade-off. Every speedup has a cost. Every delegation has a gap.

## What I'd Tell Someone Else

If you're analyzing your own AI usage:

1. **Extract your data** — It's harder than it sounds, but worth it
2. **Run privacy-preserving analysis** — Don't store raw conversations
3. **Look for patterns, not scores** — The indices are heuristics; the trends are truth
4. **Find the gaps** — Where are you delegating what you should own?
5. **Set concrete changes** — Vague improvement goals produce vague results

## The Verdict

The workstyle report was humbling. It confirmed suspicions I had and revealed blind spots I didn't.

I'm not a great debugger (low debug maturity). I don't write tests (low testing discipline). I leak sensitive data (high risky disclosure).

But I'm action-oriented (high constraint framing), multi-system (broad language distribution), and iterative (high error sharing).

The profile isn't good or bad. It's just true.

And now that it's true, I can work with it.

---

_Data: 809 conversations, 19,539 messages, 16,484 tool uses, 4 months of AI-assisted development. Analysis: privacy-preserving, k-anonymous, heuristic-based. Verdict: honest._
